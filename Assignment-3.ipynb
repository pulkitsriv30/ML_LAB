{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d61cffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Please download the dataset and ensure it's in the correct path ('house_price_data.csv').\n",
      "Fold 1: R2 Score = -0.2257\n",
      "Fold 2: R2 Score = -0.0175\n",
      "Fold 3: R2 Score = -0.1066\n",
      "Fold 4: R2 Score = -0.0535\n",
      "Fold 5: R2 Score = -0.0297\n",
      "\n",
      "--- 5-Fold Cross Validation Results ---\n",
      "   Iteration  R2_score                                               Beta\n",
      "0          1 -0.225721  [0.46638947415781284, -0.03918218712227385, 0....\n",
      "1          2 -0.017503  [0.4415054018263702, -0.042278672191861634, 0....\n",
      "2          3 -0.106604  [0.44124435447486077, -0.052574761330143695, 0...\n",
      "3          4 -0.053457  [0.4339394688781836, -0.04110638237811426, 0.0...\n",
      "4          5 -0.029712  [0.4282801295041535, -0.010667762211253422, -0...\n",
      "\n",
      "Best R2 Score: -0.0175 from Fold 2\n",
      "Best Beta (β) Matrix:\n",
      "[ 0.4415054  -0.04227867  0.02173684  0.00108627]\n",
      "\n",
      "--- Final Performance Test (70/30 split with Best Beta) ---\n",
      "R2 Score on 30% Test Data using Best Beta: -0.1334\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- STEP 1: Load the dataset ---\n",
    "# NOTE: Replace 'house_price_data.csv' with the actual file path/name of your downloaded dataset\n",
    "try:\n",
    "    df_q1 = pd.read_csv('house_price_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Please download the dataset and ensure it's in the correct path ('house_price_data.csv').\")\n",
    "    # Using a dummy dataframe for demonstration if file is not found (you should use your real file)\n",
    "    df_q1 = pd.DataFrame(np.random.rand(100, 4), columns=['Feature1', 'Feature2', 'Feature3', 'Price'])\n",
    "    # Exit or raise error if the real file is required\n",
    "\n",
    "# a) Divide the dataset into input features (X) and output variable (y)\n",
    "X = df_q1.drop('Price', axis=1) # Assuming 'Price' is the output column\n",
    "y = df_q1['Price']\n",
    "\n",
    "# b) Scale the values of input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# --- Least Square Error (LSE) Fit Function ---\n",
    "def lse_fit(X_train, y_train):\n",
    "    # Add a column of ones for the intercept term (β0)\n",
    "    X_train_b = np.c_[np.ones((len(X_train), 1)), X_train]\n",
    "    # Normal Equation: β = (X^T * X)^-1 * X^T * y\n",
    "    beta = np.linalg.inv(X_train_b.T @ X_train_b) @ X_train_b.T @ y_train\n",
    "    return beta\n",
    "\n",
    "def predict(X_data, beta):\n",
    "    # Add a column of ones for the intercept term (β0)\n",
    "    X_data_b = np.c_[np.ones((len(X_data), 1)), X_data]\n",
    "    return X_data_b @ beta\n",
    "\n",
    "# c) Divide input and output features into five folds\n",
    "# d) Run five iterations for 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores = []\n",
    "betas = []\n",
    "fold_results = []\n",
    "\n",
    "for iteration, (train_index, test_index) in enumerate(kf.split(X_scaled)):\n",
    "    # Split data into training and test sets for the current fold\n",
    "    X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Calculate beta (β) matrix using LSE fit\n",
    "    beta = lse_fit(X_train, y_train)\n",
    "    \n",
    "    # Find predicted values\n",
    "    y_pred = predict(X_test, beta)\n",
    "    \n",
    "    # Calculate R2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    r2_scores.append(r2)\n",
    "    betas.append(beta)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Iteration': iteration + 1,\n",
    "        'R2_score': r2,\n",
    "        'Beta': beta\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {iteration + 1}: R2 Score = {r2:.4f}\")\n",
    "\n",
    "# Find the best β matrix (for which R2_score is maximum)\n",
    "best_r2_index = np.argmax(r2_scores)\n",
    "best_beta = betas[best_r2_index]\n",
    "print(\"\\n--- 5-Fold Cross Validation Results ---\")\n",
    "print(pd.DataFrame(fold_results))\n",
    "print(f\"\\nBest R2 Score: {r2_scores[best_r2_index]:.4f} from Fold {best_r2_index + 1}\")\n",
    "print(f\"Best Beta (β) Matrix:\\n{best_beta}\")\n",
    "\n",
    "# e) Use the best β matrix to train regressor for 70% of data and test on 30%\n",
    "print(\"\\n--- Final Performance Test (70/30 split with Best Beta) ---\")\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Use the best beta directly for prediction on the test set (no need to retrain if using LSE)\n",
    "y_pred_final = predict(X_test_final, best_beta)\n",
    "final_r2 = r2_score(y_test_final, y_pred_final)\n",
    "\n",
    "print(f\"R2 Score on 30% Test Data using Best Beta: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e8b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 55 (56%), Validation size: 15 (14%), Test size: 30 (30%)\n",
      "\n",
      "Training with Learning Rate (α) = 0.001...\n",
      "  R2_Validation: -4.8947, R2_Test: -4.9527\n",
      "\n",
      "Training with Learning Rate (α) = 0.01...\n",
      "  R2_Validation: -0.1781, R2_Test: -0.3133\n",
      "\n",
      "Training with Learning Rate (α) = 0.1...\n",
      "  R2_Validation: -0.1781, R2_Test: -0.3133\n",
      "\n",
      "Training with Learning Rate (α) = 1...\n",
      "  R2_Validation: -85447766317579007391777231043577789173872963601049621298126591879036562320453573992875606291464596037245847556062355007610899027043551420253107565734110441855362503895466349078719801010225936360264340245960401309660673974573186793951814100257748015709427356139520.0000, R2_Test: -62289890294800771729119665240167488202295134601862283659957937503880661462444122069934743594408782444553167318793540933131022049913603047573728783001109931615984398442357197102473546270402530309808742306548745420105849394243210301797941656191167381343291920351232.0000\n",
      "\n",
      "--- Gradient Descent Results Summary ---\n",
      "   Learning_Rate  R2_Validation        R2_Test\n",
      "0          0.001  -4.894723e+00  -4.952745e+00\n",
      "1          0.010  -1.781448e-01  -3.132504e-01\n",
      "2          0.100  -1.781448e-01  -3.132505e-01\n",
      "3          1.000 -8.544777e+262 -6.228989e+262\n",
      "\n",
      "Best Learning Rate (based on R2_Validation): **0.01**\n",
      "Best R2_Validation: -0.1781\n",
      "R2_Test with Best Beta: **-0.3133**\n",
      "Best Regression Coefficients (Beta):\n",
      "[ 0.46182366  0.07097901  0.05729096 -0.03068526]\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "# Reuse the data loaded and scaled in Q1\n",
    "# X_scaled, y are already defined from Q1\n",
    "\n",
    "# --- STEP 1: Split the dataset (56% Training, 14% Validation, 30% Test) ---\n",
    "# Split 70% (Train+Validation) and 30% (Test) first\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split 70% (X_temp, y_temp) into 80% Training and 20% Validation\n",
    "# 0.7 * 0.8 = 0.56 (56% Training)\n",
    "# 0.7 * 0.2 = 0.14 (14% Validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=(0.14/0.7), random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(X_train)} (56%), Validation size: {len(X_val)} (14%), Test size: {len(X_test)} (30%)\")\n",
    "\n",
    "# --- Gradient Descent Optimization Function ---\n",
    "def gradient_descent(X_train, y_train, learning_rate, n_iterations):\n",
    "    # Prepare X_train by adding intercept term\n",
    "    X_b = np.c_[np.ones((len(X_train), 1)), X_train]\n",
    "    y_train_array = y_train.values.reshape(-1, 1) # Ensure y is a column vector\n",
    "    \n",
    "    # Initialize random beta vector (intercept + features)\n",
    "    beta = np.random.randn(X_b.shape[1], 1)\n",
    "    m = len(y_train)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Calculate gradients (partial derivatives of the cost function)\n",
    "        gradients = (2/m) * X_b.T @ (X_b @ beta - y_train_array)\n",
    "        # Update beta\n",
    "        beta = beta - learning_rate * gradients\n",
    "        \n",
    "    return beta.flatten() # Return as a 1D array\n",
    "\n",
    "def predict_gd(X_data, beta):\n",
    "    # Add intercept column\n",
    "    X_b = np.c_[np.ones((len(X_data), 1)), X_data]\n",
    "    return X_b @ beta\n",
    "\n",
    "# --- Experiment with different learning rates ---\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "n_iterations = 1000\n",
    "gd_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with Learning Rate (α) = {lr}...\")\n",
    "    \n",
    "    # Compute regression coefficients (beta)\n",
    "    beta_gd = gradient_descent(X_train, y_train, lr, n_iterations)\n",
    "    \n",
    "    # Compute R2_score for Validation Set\n",
    "    y_val_pred = predict_gd(X_val, beta_gd)\n",
    "    r2_val = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Compute R2_score for Test Set\n",
    "    y_test_pred = predict_gd(X_test, beta_gd)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    gd_results.append({\n",
    "        'Learning_Rate': lr,\n",
    "        'R2_Validation': r2_val,\n",
    "        'R2_Test': r2_test,\n",
    "        'Beta': beta_gd\n",
    "    })\n",
    "    \n",
    "    print(f\"  R2_Validation: {r2_val:.4f}, R2_Test: {r2_test:.4f}\")\n",
    "\n",
    "# Find the best value of regression coefficients (based on maximum R2_score on Validation Set)\n",
    "results_df = pd.DataFrame(gd_results)\n",
    "best_index = results_df['R2_Validation'].idxmax()\n",
    "best_lr_result = results_df.iloc[best_index]\n",
    "\n",
    "print(\"\\n--- Gradient Descent Results Summary ---\")\n",
    "print(results_df[['Learning_Rate', 'R2_Validation', 'R2_Test']])\n",
    "\n",
    "print(f\"\\nBest Learning Rate (based on R2_Validation): **{best_lr_result['Learning_Rate']}**\")\n",
    "print(f\"Best R2_Validation: {best_lr_result['R2_Validation']:.4f}\")\n",
    "print(f\"R2_Test with Best Beta: **{best_lr_result['R2_Test']:.4f}**\")\n",
    "print(f\"Best Regression Coefficients (Beta):\\n{best_lr_result['Beta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848d580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Categorical Encoding...\n",
      "\n",
      "--- 5. Standard Linear Regression (70/30 Split) ---\n",
      "R2 Score on Test Set (Standard LR): **0.8044**\n",
      "\n",
      "--- 6. PCA-based Linear Regression ---\n",
      "Number of components retaining 95% variance: **16** (down from 29 features)\n",
      "R2 Score on Test Set (PCA-based LR): **0.7578**\n",
      "\n",
      "Performance Change (PCA R2 - Standard LR R2): -0.0467\n",
      "Conclusion: No, PCA decomposition **did not lead to a performance improvement** on the test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- STEP 1: Load the dataset and replace '?' with NaN ---\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
    "column_names = [\n",
    "    \"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\", \n",
    "    \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\", \"wheel_base\", \n",
    "    \"length\", \"width\", \"height\", \"curb_weight\", \"engine_type\", \"num_cylinders\", \n",
    "    \"engine_size\", \"fuel_system\", \"bore\", \"stroke\", \"compression_ratio\", \n",
    "    \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"\n",
    "]\n",
    "df_q3 = pd.read_csv(url, header=None, names=column_names)\n",
    "df_q3 = df_q3.replace('?', np.nan)\n",
    "\n",
    "# --- STEP 2: Handle Missing Values ---\n",
    "# Convert columns that should be numeric but contain '?' to float\n",
    "cols_to_convert = [\"normalized_losses\", \"bore\", \"stroke\", \"horsepower\", \"peak_rpm\", \"price\"]\n",
    "for col in cols_to_convert:\n",
    "    df_q3[col] = pd.to_numeric(df_q3[col])\n",
    "\n",
    "# Replace all NaN values with central tendency imputation (mean for numeric, mode for object)\n",
    "for col in df_q3.columns:\n",
    "    if df_q3[col].dtype == 'object':\n",
    "        # Mode imputation for categorical\n",
    "        df_q3[col] = df_q3[col].fillna(df_q3[col].mode()[0])\n",
    "    else:\n",
    "        # Mean imputation for numeric\n",
    "        df_q3[col] = df_q3[col].fillna(df_q3[col].mean())\n",
    "\n",
    "# Drop the rows with NaN values in the price column (although imputation was used, this is a safety step)\n",
    "df_q3.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "# --- STEP 3: Convert non-numeric values to numeric ---\n",
    "print(\"Applying Categorical Encoding...\")\n",
    "\n",
    "# (i) Convert number names to figures\n",
    "num_map = {'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'eight': 8, 'twelve': 12}\n",
    "df_q3['num_doors'] = df_q3['num_doors'].map({'two': 2, 'four': 4}).fillna(df_q3['num_doors']) # Fill missing due to mode being applied\n",
    "df_q3['num_cylinders'] = df_q3['num_cylinders'].map(num_map)\n",
    "\n",
    "# (ii) Dummy Encoding Scheme (One-Hot Encoding)\n",
    "df_q3 = pd.get_dummies(df_q3, columns=['body_style', 'drive_wheels'], drop_first=True)\n",
    "\n",
    "# (iii) Label Encoding Scheme\n",
    "label_encode_cols = ['make', 'aspiration', 'engine_location', 'fuel_type']\n",
    "le = LabelEncoder()\n",
    "for col in label_encode_cols:\n",
    "    df_q3[col] = le.fit_transform(df_q3[col])\n",
    "\n",
    "# (iv) For fuel_system\n",
    "df_q3['fuel_system'] = df_q3['fuel_system'].apply(lambda x: 1 if 'pfi' in str(x).lower() else 0)\n",
    "\n",
    "# (v) For engine_type\n",
    "df_q3['engine_type'] = df_q3['engine_type'].apply(lambda x: 1 if 'ohc' in str(x).lower() else 0)\n",
    "\n",
    "# --- STEP 4: Divide and Scale ---\n",
    "X_q3 = df_q3.drop('price', axis=1)\n",
    "y_q3 = df_q3['price']\n",
    "\n",
    "# Scale all input features\n",
    "scaler_q3 = StandardScaler()\n",
    "X_q3_scaled = scaler_q3.fit_transform(X_q3)\n",
    "X_q3_scaled = pd.DataFrame(X_q3_scaled, columns=X_q3.columns)\n",
    "\n",
    "# Split data for training/testing\n",
    "X_train_q3, X_test_q3, y_train_q3, y_test_q3 = train_test_split(\n",
    "    X_q3_scaled, y_q3, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- STEP 5: Train a linear regressor (Original Data) ---\n",
    "print(\"\\n--- 5. Standard Linear Regression (70/30 Split) ---\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_q3, y_train_q3)\n",
    "y_pred_lr = lr_model.predict(X_test_q3)\n",
    "r2_lr = r2_score(y_test_q3, y_pred_lr)\n",
    "print(f\"R2 Score on Test Set (Standard LR): **{r2_lr:.4f}**\")\n",
    "\n",
    "# --- STEP 6: PCA and then train a linear regressor ---\n",
    "print(\"\\n--- 6. PCA-based Linear Regression ---\")\n",
    "\n",
    "# Determine optimal number of components for 95% variance retention\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train_q3)\n",
    "X_train_pca = pca.transform(X_train_q3)\n",
    "X_test_pca = pca.transform(X_test_q3)\n",
    "\n",
    "n_components = pca.n_components_\n",
    "print(f\"Number of components retaining 95% variance: **{n_components}** (down from {X_q3.shape[1]} features)\")\n",
    "\n",
    "# Train a linear regressor on reduced data\n",
    "lr_pca_model = LinearRegression()\n",
    "lr_pca_model.fit(X_train_pca, y_train_q3)\n",
    "y_pred_pca = lr_pca_model.predict(X_test_pca)\n",
    "r2_pca = r2_score(y_test_q3, y_pred_pca)\n",
    "print(f\"R2 Score on Test Set (PCA-based LR): **{r2_pca:.4f}**\")\n",
    "\n",
    "# Performance comparison\n",
    "performance_improvement = r2_pca - r2_lr\n",
    "print(f\"\\nPerformance Change (PCA R2 - Standard LR R2): {performance_improvement:.4f}\")\n",
    "\n",
    "if r2_pca > r2_lr:\n",
    "    print(\"Conclusion: Yes, PCA decomposition **led to a performance improvement** on the test set.\")\n",
    "else:\n",
    "    print(\"Conclusion: No, PCA decomposition **did not lead to a performance improvement** on the test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
