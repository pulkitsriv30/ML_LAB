{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376d1c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Books... (Paginating through all pages)\n",
      "Finished. Last successful page: 50\n",
      "✅ Q1 Complete. Scraped 1000 books and saved to books.csv.\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "books_data = []\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "page_num = 1\n",
    "\n",
    "print(\"Scraping Books... (Paginating through all pages)\")\n",
    "\n",
    "while True:\n",
    "    url = base_url.format(page_num)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        # Check if the page exists; break when 404 is encountered (end of site)\n",
    "        print(f\"Finished. Last successful page: {page_num - 1}\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    for book in books:\n",
    "        # 1. Title [cite: 6]\n",
    "        title = book.h3.a['title']\n",
    "        \n",
    "        # 2. Price [cite: 7]\n",
    "        price = book.find('p', class_='price_color').text.strip().replace('£', '')\n",
    "        \n",
    "        # 3. Availability [cite: 8]\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "        \n",
    "        # 4. Star Rating [cite: 9]\n",
    "        rating_class = book.p['class'][1]\n",
    "        rating_map = {'One': 'One', 'Two': 'Two', 'Three': 'Three', 'Four': 'Four', 'Five': 'Five'}\n",
    "        star_rating = rating_map.get(rating_class, 'Unknown')\n",
    "        \n",
    "        books_data.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Availability': availability,\n",
    "            'Star Rating': star_rating\n",
    "        })\n",
    "\n",
    "    page_num += 1\n",
    "\n",
    "# Store in DataFrame and export to CSV [cite: 10]\n",
    "df_books = pd.DataFrame(books_data)\n",
    "df_books.to_csv('books.csv', index=False)\n",
    "\n",
    "print(f\"✅ Q1 Complete. Scraped {len(df_books)} books and saved to books.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a94a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q2 Complete. Scraped 0 movies and saved to imdb_top250.csv.\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: Set the path to your ChromeDriver or use Service for newer versions.\n",
    "# This assumes ChromeDriver is in your system PATH or use a specific path/service.\n",
    "# For minimal code, we assume ChromeDriver is accessible/Service is set up.\n",
    "# driver = webdriver.Chrome(service=Service('/path/to/chromedriver')) \n",
    "driver = webdriver.Chrome() # Assumes driver is in PATH\n",
    "\n",
    "url_imdb = \"https://www.imdb.com/chart/top/\"\n",
    "driver.get(url_imdb)\n",
    "\n",
    "# Find the table containing all movie entries\n",
    "table = driver.find_element(By.CSS_SELECTOR, 'ul.ipc-metadata-list')\n",
    "movies = table.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "imdb_data = []\n",
    "\n",
    "for movie in movies:\n",
    "    try:\n",
    "        # 1. Rank & 2. Movie Title [cite: 14, 15]\n",
    "        # Locate the title element which contains the rank and title\n",
    "        title_element = movie.find_element(By.CSS_SELECTOR, 'h3.ipc-title__text')\n",
    "        # e.g., \"1. The Shawshank Redemption\"\n",
    "        \n",
    "        # Split rank and title (split at the first '.')\n",
    "        rank_title = title_element.text\n",
    "        rank, title = rank_title.split('.', 1)\n",
    "        rank = rank.strip()\n",
    "        title = title.strip()\n",
    "\n",
    "        # 3. Year of Release [cite: 16] (often under the title)\n",
    "        # Use find_elements to handle cases where some items might not have a year or rating\n",
    "        year_element = movie.find_elements(By.CSS_SELECTOR, 'span.cli-title-metadata-item:nth-child(1)')\n",
    "        year = year_element[0].text if year_element else 'N/A'\n",
    "\n",
    "        # 4. IMDB Rating [cite: 17]\n",
    "        rating_element = movie.find_element(By.CSS_SELECTOR, 'span.ipc-rating-star--imdb')\n",
    "        rating = rating_element.text.split('\\n')[0].strip() # Takes the \"8.9\" part\n",
    "\n",
    "        imdb_data.append({\n",
    "            'Rank': rank,\n",
    "            'Movie Title': title,\n",
    "            'Year of Release': year,\n",
    "            'IMDB Rating': rating\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # Skip if a list item doesn't conform to the expected structure (e.g., ads, breaks)\n",
    "        continue\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Store in DataFrame and export to CSV [cite: 18]\n",
    "df_imdb = pd.DataFrame(imdb_data)\n",
    "df_imdb.to_csv('imdb_top250.csv', index=False)\n",
    "\n",
    "print(f\"✅ Q2 Complete. Scraped {len(df_imdb)} movies and saved to imdb_top250.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed35d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Q3 Complete. Scraped 0 weather records and saved to weather.csv.\n"
     ]
    }
   ],
   "source": [
    "#q3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url_weather = \"https://www.timeanddate.com/weather/\"\n",
    "response = requests.get(url_weather)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# The cities and weather data are typically in a table on this page\n",
    "# Find the main table for world cities (based on typical timeanddate structure)\n",
    "table = soup.find('table', class_='wtt') # Use table class for stability\n",
    "\n",
    "weather_data = []\n",
    "\n",
    "if table:\n",
    "    # Iterate over rows in the table body, skipping the header\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        \n",
    "        if len(cells) >= 3: # Expect at least City, Temp, Condition\n",
    "            # 1. City Name [cite: 22] (often in the first cell, sometimes a link)\n",
    "            city_name = cells[0].text.strip()\n",
    "            \n",
    "            # 2. Temperature [cite: 23] (often in the second cell)\n",
    "            temperature = cells[1].text.strip()\n",
    "            \n",
    "            # 3. Weather Condition [cite: 24] (often in the third cell)\n",
    "            condition = cells[2].text.strip()\n",
    "\n",
    "            weather_data.append({\n",
    "                'City Name': city_name,\n",
    "                'Temperature': temperature,\n",
    "                'Weather Condition': condition\n",
    "            })\n",
    "\n",
    "# Store in DataFrame and export to CSV [cite: 25]\n",
    "df_weather = pd.DataFrame(weather_data)\n",
    "df_weather.to_csv('weather.csv', index=False)\n",
    "\n",
    "print(f\"✅ Q3 Complete. Scraped {len(df_weather)} weather records and saved to weather.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
