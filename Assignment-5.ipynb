{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05c0ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Q1: Ridge Regression (Gradient Descent) ---\n",
      "Regularization Parameter (λ): 1e-05\n",
      "Best Learning Rate (α): 0.01\n",
      "Maximum R2 Score: 0.0825\n",
      "Minimum Cost Function Value: 384.9441\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- STEP 1: Generate Highly Correlated Dataset ---\n",
    "np.random.seed(42)\n",
    "N = 500\n",
    "X = np.random.randn(N, 7) # 7 features\n",
    "# Create correlation: Feature 1 is highly dependent on F0\n",
    "X[:, 1] = X[:, 0] * 0.9 + np.random.randn(N) * 0.1\n",
    "X[:, 2] = X[:, 0] * 0.8 + X[:, 3] * 0.5\n",
    "X[:, 3] = X[:, 4] * 0.7 + np.random.randn(N) * 0.3\n",
    "# Target variable y = 1 + 2*X0 + 3*X1 + ... + noise\n",
    "true_betas = np.array([2, 3, -1, 0.5, 4, -2, 1])\n",
    "y = 1 + X @ true_betas + np.random.randn(N) * 20\n",
    "\n",
    "# Scale the data and split\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# --- STEP 2: Ridge Regression with Gradient Descent Implementation ---\n",
    "class RidgeRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, lambda_param=1e-5):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lambda_param = lambda_param\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.cost_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        # Initialize coefficients (weights + intercept) with small random values\n",
    "        self.coef_ = np.random.randn(n, 1) * 0.01\n",
    "        self.intercept_ = np.random.randn(1, 1) * 0.01\n",
    "        \n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            y_pred = X @ self.coef_ + self.intercept_\n",
    "            \n",
    "            # Cost function (Mean Squared Error + L2 Regularization)\n",
    "            cost = (1/m) * np.sum((y_pred - y)**2) + self.lambda_param * np.sum(self.coef_**2)\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "            # Gradients (with L2 term: 2*lambda*w)\n",
    "            d_coef = (2/m) * (X.T @ (y_pred - y)) + 2 * self.lambda_param * self.coef_\n",
    "            d_intercept = (2/m) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Gradient clipping to prevent overflow\n",
    "            d_coef = np.clip(d_coef, -10, 10)\n",
    "            d_intercept = np.clip(d_intercept, -10, 10)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.coef_ -= self.lr * d_coef\n",
    "            self.intercept_ -= self.lr * d_intercept\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "\n",
    "# --- STEP 3: Experiment and Find Best Parameters ---\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "lambda_param = 1e-5\n",
    "best_params = {'R2': -np.inf, 'LR': None, 'Cost': np.inf}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = RidgeRegressionGD(learning_rate=lr, n_iterations=10000, lambda_param=lambda_param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    min_cost = model.cost_history[-1]\n",
    "    \n",
    "    if r2 > best_params['R2'] or (r2 == best_params['R2'] and min_cost < best_params['Cost']):\n",
    "        best_params['R2'] = r2\n",
    "        best_params['LR'] = lr\n",
    "        best_params['Cost'] = min_cost\n",
    "\n",
    "print(\"--- Q1: Ridge Regression (Gradient Descent) ---\")\n",
    "print(f\"Regularization Parameter (λ): {lambda_param}\")\n",
    "print(f\"Best Learning Rate (α): {best_params['LR']}\")\n",
    "print(f\"Maximum R2 Score: {best_params['R2']:.4f}\")\n",
    "print(f\"Minimum Cost Function Value: {best_params['Cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b947563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Q2: Regression Model Performance on Hitters Dataset ---\n",
      "| Model             |   R2 Score on Test Set |\n",
      "|:------------------|-----------------------:|\n",
      "| Ridge Regression  |               0.402107 |\n",
      "| LASSO Regression  |               0.397415 |\n",
      "| Linear Regression |               0.380623 |\n",
      "\n",
      "Best Performing Model: **Ridge Regression**\n",
      "Explanation: Linear Regression is the baseline. **Ridge** adds L2 regularization, preventing coefficients from becoming too large (good for multicollinearity). **LASSO** adds L1 regularization, forcing some coefficients to zero (performing feature selection). The best model depends on the dataset's characteristics, specifically the degree of multicollinearity and the number of irrelevant features.\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# --- STEP (a): Load and Pre-process Data ---\n",
    "try:\n",
    "    df_h = pd.read_csv('Hitters.csv').dropna()\n",
    "except FileNotFoundError:\n",
    "    print(\"NOTE: 'Hitters.csv' not found. Generating synthetic data for demonstration.\")\n",
    "    # Create synthetic Hitters-like dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 263\n",
    "    df_h = pd.DataFrame({\n",
    "        'AtBat': np.random.randint(100, 700, n_samples),\n",
    "        'Hits': np.random.randint(10, 200, n_samples),\n",
    "        'HmRun': np.random.randint(0, 50, n_samples),\n",
    "        'Runs': np.random.randint(10, 150, n_samples),\n",
    "        'RBI': np.random.randint(10, 150, n_samples),\n",
    "        'Walks': np.random.randint(5, 100, n_samples),\n",
    "        'Years': np.random.randint(1, 25, n_samples),\n",
    "        'CAtBat': np.random.randint(100, 10000, n_samples),\n",
    "        'CHits': np.random.randint(10, 3000, n_samples),\n",
    "        'CHmRun': np.random.randint(0, 500, n_samples),\n",
    "        'CRuns': np.random.randint(10, 2000, n_samples),\n",
    "        'CRBI': np.random.randint(10, 2000, n_samples),\n",
    "        'CWalks': np.random.randint(5, 1500, n_samples),\n",
    "        'League': np.random.choice(['A', 'N'], n_samples),\n",
    "        'Division': np.random.choice(['E', 'W'], n_samples),\n",
    "        'PutOuts': np.random.randint(0, 1500, n_samples),\n",
    "        'Assists': np.random.randint(0, 500, n_samples),\n",
    "        'Errors': np.random.randint(0, 30, n_samples),\n",
    "        'Salary': np.random.randint(50, 2500, n_samples) * 1000\n",
    "    })\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "y_h = df_h['Salary']\n",
    "X_h = df_h.drop('Salary', axis=1)\n",
    "\n",
    "# Identify feature types\n",
    "numerical_features = X_h.select_dtypes(include=np.number).columns\n",
    "categorical_features = X_h.select_dtypes(include='object').columns\n",
    "\n",
    "# Preprocessing Pipeline (Scaling + One-Hot Encoding)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- STEP (b): Separate and Scale (Handled by Pipeline) ---\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_h, y_h, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- STEP (c) & (d): Fit and Evaluate Models ---\n",
    "alpha_param = 0.5748\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=alpha_param, random_state=42),\n",
    "    \"LASSO Regression\": Lasso(alpha=alpha_param, random_state=42, max_iter=5000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Create and train the full pipeline\n",
    "    full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('regressor', model)])\n",
    "    \n",
    "    full_pipeline.fit(X_train_h, y_train_h)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred_h = full_pipeline.predict(X_test_h)\n",
    "    r2 = r2_score(y_test_h, y_pred_h)\n",
    "    results[name] = r2\n",
    "\n",
    "print(\"\\n--- Q2: Regression Model Performance on Hitters Dataset ---\")\n",
    "results_df = pd.DataFrame(results.items(), columns=['Model', 'R2 Score on Test Set'])\n",
    "print(results_df.sort_values(by='R2 Score on Test Set', ascending=False).to_markdown(index=False))\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"\\nBest Performing Model: **{best_model}**\")\n",
    "print(\"Explanation: Linear Regression is the baseline. **Ridge** adds L2 regularization, preventing coefficients from becoming too large (good for multicollinearity). **LASSO** adds L1 regularization, forcing some coefficients to zero (performing feature selection). The best model depends on the dataset's characteristics, specifically the degree of multicollinearity and the number of irrelevant features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1599d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Q3: RidgeCV and LassoCV Results ---\n",
      "Dataset Used: California Housing (substitute for deprecated Boston Housing)\n",
      "--- RidgeCV ---\n",
      "Optimal Alpha (λ): 1.00000\n",
      "Test Set MSE: 0.5305\n",
      "\n",
      "--- LassoCV ---\n",
      "Optimal Alpha (λ): 0.00285\n",
      "Test Set MSE: 0.5285\n"
     ]
    }
   ],
   "source": [
    "#q3\n",
    "from sklearn.datasets import fetch_california_housing # Using California Housing as Boston is deprecated\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "# NOTE: The Boston dataset (load_boston) is deprecated. Using fetch_california_housing as a modern substitute.\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X_cv, y_cv = data.data, data.target\n",
    "\n",
    "# Split and Scale\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(\n",
    "    X_cv, y_cv, test_size=0.3, random_state=42\n",
    ")\n",
    "scaler_cv = StandardScaler()\n",
    "X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "X_test_cv_scaled = scaler_cv.transform(X_test_cv)\n",
    "\n",
    "# Set up candidate alphas\n",
    "alphas = np.logspace(-4, 0, 100) # 100 alpha values from 0.0001 to 1\n",
    "\n",
    "# --- Ridge Cross Validation (RidgeCV) ---\n",
    "# cv=None uses Generalized Cross-Validation (faster LOOCV approximation)\n",
    "ridge_cv = RidgeCV(alphas=alphas)\n",
    "ridge_cv.fit(X_train_cv_scaled, y_train_cv)\n",
    "y_pred_ridge = ridge_cv.predict(X_test_cv_scaled)\n",
    "mse_ridge = mean_squared_error(y_test_cv, y_pred_ridge)\n",
    "\n",
    "# --- Lasso Cross Validation (LassoCV) ---\n",
    "# cv=5 uses 5-fold cross-validation\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_train_cv_scaled, y_train_cv)\n",
    "y_pred_lasso = lasso_cv.predict(X_test_cv_scaled)\n",
    "mse_lasso = mean_squared_error(y_test_cv, y_pred_lasso)\n",
    "\n",
    "print(\"\\n--- Q3: RidgeCV and LassoCV Results ---\")\n",
    "print(f\"Dataset Used: California Housing (substitute for deprecated Boston Housing)\")\n",
    "print(\"--- RidgeCV ---\")\n",
    "print(f\"Optimal Alpha (λ): {ridge_cv.alpha_:.5f}\")\n",
    "print(f\"Test Set MSE: {mse_ridge:.4f}\")\n",
    "\n",
    "print(\"\\n--- LassoCV ---\")\n",
    "print(f\"Optimal Alpha (λ): {lasso_cv.alpha_:.5f}\")\n",
    "print(f\"Test Set MSE: {mse_lasso:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
